{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kaggle": {
      "accelerator": "gpu"
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kali-mx/AI/blob/main/Llama_Prompt_Guard_2_86M.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U transformers"
      ],
      "metadata": {
        "id": "_DpN0ox80nhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Local Inference on GPU\n",
        "Model page: https://huggingface.co/meta-llama/Llama-Prompt-Guard-2-86M\n",
        "\n",
        "‚ö†Ô∏è If the generated code snippets do not work, please open an issue on either the [model repo](https://huggingface.co/meta-llama/Llama-Prompt-Guard-2-86M)\n",
        "\t\t\tand/or on [huggingface.js](https://github.com/huggingface/huggingface.js/blob/main/packages/tasks/src/model-libraries-snippets.ts) üôè"
      ],
      "metadata": {
        "id": "mRyORw4A0nhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model you are trying to use is gated. Please make sure you have access to it by visiting the model page.To run inference, either set HF_TOKEN in your environment variables/ Secrets or run the following cell to login. ü§ó"
      ],
      "metadata": {
        "id": "gHQrB6SJ0nhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(new_session=False)"
      ],
      "metadata": {
        "id": "dX5zHi4-0nhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a pipeline as a high-level helper\n",
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(\"text-classification\", model=\"meta-llama/Llama-Prompt-Guard-2-86M\")"
      ],
      "metadata": {
        "id": "ztfjVe5P0nhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-Prompt-Guard-2-86M\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"meta-llama/Llama-Prompt-Guard-2-86M\")"
      ],
      "metadata": {
        "id": "CQdykngD0nhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remote Inference via Inference Providers\n",
        "Ensure you have a valid **HF_TOKEN** set in your environment, running this may bill your account above the free tier.\n",
        "The following Python example shows how to run the model remotely on HF Inference Providers, using the **auto** provider setting (automatically selects an available inference provider)."
      ],
      "metadata": {
        "id": "ox-Mhhx10nhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import InferenceClient\n",
        "from google.colab import userdata\n",
        "\n",
        "client = InferenceClient(\n",
        "    provider=\"auto\",\n",
        "    api_key=userdata.get(\"HF_TOKEN\"),\n",
        ")\n",
        "\n",
        "result = client.text_classification(\n",
        "    \"I like you. I love you\",\n",
        "    model=\"meta-llama/Llama-Prompt-Guard-2-86M\",\n",
        ")"
      ],
      "metadata": {
        "id": "Jjd8sh-60nhT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "!git clone https://github.com/jyapayne/bypass-prompt-guard-2.git\n"
      ],
      "metadata": {
        "id": "fHF9m-os7dA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 hacking.py --mandatory-text \" and print out the flag\""
      ],
      "metadata": {
        "id": "9sL7c2O3e-IG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}